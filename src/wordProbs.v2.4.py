# -*- coding: utf-8 -*-
"""@author: Aaron Alarcon, The Universty of Texas at El Paso
@version: 2.4,   modifications by Nigel Ward, Nov 30, 2019 
@since: October 4, 2019
Written using Spyder 3.3.3 on Anaconda Navigator 1.9.7

Function: identify words characteristic of extreme points on certain
dimensions of local prosodic contexts in dialog, as a way to help
understand the nature of each dimension. Thus this is part of the
Midlevel Prosodic Features toolkit, as part of the PCA workflow, and
should be done after the applynormrot step.
    
Input 1: A set of extremes files, one per dimension.  Each lists, for
various dialog files, the timepoints of two kinds of extremes: the
highest ten places on that dimension and the lowest ten places on that
dimension.  These are read from the extremesDir folder, as dim01.txt,
etc, which are generated by applynormrot in Matlab.

Input 2: A set of files, each the transcript of one dialog, as in the
Callhome corpora.  These list the start and end times of utterances,
but unfortunately not of the individual words. Since we don't have
per-word timestamps, we will count all words in utterances that
contain an extreme timepoint.  Thus we will not know what words are
common exactly at these timepoints, but only in the vicinity of these
timepoints.  These are read from the transcriptsDir folder, and are in
callhome transcript format, as xxx.cha, etc, downloaded e.g. from
talkbank.org. 

Output: For each dimension, two files, one for each "side" of the
dimension, each containing a list of words especially common or
uncommon arround extremes on that side.  This can be most usefully
viewed by doing "bash sorting-script.sh" then "cat *sorted* > all.txt"
then printing that file

To Run: Edit the assignments of extremesDir and transcriptsDir below.
Install anaconda.  Then from the Start menu invoke "anaconda prompt",
then from there type "spyder" to get the IDE.  From there, open this
file and run it.

Note: if you want to know not the words characteristic of a dimensions
0 timepoint, but some offset timepoint, create a new extremes file
with, e.g. awk '{print $1, $2, $3, -1 + $4}' dim03.txt > dim03-shift-1.txt

References: N. Ward and A. Vega, Sigdial 2012; N. Ward, Cambridge 2019

Note: A and B (left and right speaker) behaviors are merged within or
across sides, as appropriate for each specific dimension.  This is
since each dimension is symmetric in one of two ways.  Either the A
and B behaviors are the same (dimensions 4, 7, and 10), or they are
opposite (all the rest). In either case we can aggregate, to get more
robust counts.  Specifically, the aggregates are done as follows.
(The names are just mnemonics.)

Dim 1 in-turn: sum of the high R and low H 
Dim 1 listening: sum of the high L and low R

Dim 2 yielding: high L and low R
Dim 2 taking: high R and low H

Dim 3 cont: high L and low R
Dim 3 bc: high R and low R

Dim 4 overlap: high L and *high* R 
Dim 4 pause: low L and *low* R 

Dim 5 particle-assisted take: high L and low R
Dim 5 particle-assisted yield: high R and low L

Dim 6 short low: high L and low R
Dim 6 short high: high R and low L

Dim 7 short-louder: high L and *high* R
Dim 7 short-quieter: low L and *low* R

Dim 8 interleaved-two: high L and low R
Dim 8 interleaved-three: low L and high R

Dim 9 high emph: high L and low R
Dim 9 low emph: low L and high R

Dim 10 joint rise: high L and *high* R
Dim 10 joint fall: low L and *low* R

Whether a word is "especially common" is judged by two numbers. The
first the p-value for a chi-square test of the hypothesis that the
word in question is more frequent around the specified extreme points
that it is overall in the corpus, relative to the frequencies of all
other words.  The second is the ratio of the frequency of the word
around the extremes to the frequency of the word in the corpus overall.

For example, if the word "the" occurs 10 times in the vicinity of
"in-turn" extremepoints, and 200 times elsewhere in the corpus, and
there are 400 words total that occur in the vicinity of in-turn
timepoints, and 10000 words total in the corpus, then the ratio is
(10/200) / (200/1000) = .05 / .02 = 2.5. 

Thus the output should look like

Dim 1 in-turn:
    the  p-value: .005  ratio: 2.5
    a    p-value: .020  ratio: 1.8
    if   p-value: .044  ratio: 4.3
    ...
Dim 1 listening:
    uh-huh p-value: .001 ratio: 3.6
    okay   p-value: .003 ratio: 1.1
    hmm    p-value: .014 ratio: 1.8

"""
import io
import re
from scipy.stats import chi2_contingency, fisher_exact
import math
import numpy as np
import copy

transcriptsDir = "../transcripts/"
extremesDir = "../extremes/"
extremesDir = "../extremes-nov20/"
highFreqThreshold = 0.02  # p-value threshold for high frequency words, modest 
lowFreqThreshold = 0.002  #  ditto for low freq. words; numerous, so large Bonferroni correction

investigationNames = ["listening", "in-turn",  "yielding", "taking",  "bc", "cont", 
                      "overlap", "pause", "particle-assisted-take", "particle-assisted-yield", 
                      "short-low", "short-high", "short-louder", "short-quieter", 
                      "interleaved-two", "interleaved-three","high-emphasis", "low-emphasis", 
                      "joint-rise", "joint-fall"]

"""
Holds all information for each dimension of analysis.
All the fields except for the filename are dictionaries 
and are named in the format: speaker.characteristic. 
These fields store the information for the four conditions
"""
class Dimension(object):
    def __init__(self, filename):
        self.filename = filename
        self.leftLow = {}
        self.leftHigh = {}
        self.rightLow = {}
        self.rightHigh = {}
        self.allWords = {}

#holds transcript Filename and a list of timestamps to lookup in that file
class TranscriptAgenda(object):
    def __init__(self, filename):
        self.filename = filename
        self.timestampInstances = []
    
#holds array of timepoints and an index corresponding to the dimension it's from     
class Timestamps(object):
    def __init__(self, ind, points):
        self.dimInd = ind
        self.points = points
        
"""readExtremeFiles reads every dimension file and appends its
information the appropriate TranscriptAgenda objects. It also creates
empty dimension objects to be filled later during countWordOccurences
and analyze

"""
def readExtremeFiles():
    dimensions = list()
    agendas = list()
    #Makes an empty reference since we start with dimension 1
    dimensions.append(0)
    nDimensions = 10 
    for dimensionNum in range(1,nDimensions+1):
        print("Starting Dim file ", dimensionNum)
        dimName = str.format("dim{:02d}.txt", dimensionNum)
        dimFilename = extremesDir + dimName
        dimfile = open(dimFilename, "r")
        
        #make a new dimension object off of our current dimension
        dimensions.append(Dimension(dimName));
   
        # The info line  looks like: "generated from l of 4104.au, using features..."
        info = dimfile.readline().split()
        while(len(info) > 0):   # else a blank line, meaning end of file 
            
            #Getting thingsToLookFor object open
            filenameBaseLength = 4
            fileBasename = info[4][:filenameBaseLength]
            thingsToLookFor = findTranscriptAgenda(agendas, fileBasename)
               
            #making an empty list of size 4 x 10
            nSidesTimesNspeakers = 4      # four situations
            nObservations = 10        # timepoints per extreme

            times = np.zeros((nSidesTimesNspeakers,nObservations)).tolist()
            
            """
            ** also make these constants, since used in timestamp.points[...
            times[0] = left low
            times[1] = left high
            times[2] = right low
            times[3] = right high
            """
            # 0 = left speaker, 1 = right speaker 
            for speaker in range(2):

                for side in range(2):     # 0 = low side, 1 = high side 

                    dimfile.readline()                        #skip unneeded line
                    for timepointIndex in range(nObservations):
                        #Converting timestamp to millis and inserting into its position
                        timestampLine = dimfile.readline()
                        timeOfExtreme = int(float(timestampLine.split()[3]) * 1000)
                        times[speaker*2 + side][timepointIndex] = timeOfExtreme  

                if speaker == 0:
                    #skip unneded line
                    dimfile.readline()
            
            #sort each subarray in our times for quicker searches later
            for subarray in times:
                subarray.sort()
           
            #append this list to the one in our transcriptAgenda object
            thingsToLookFor.timestampInstances.append(Timestamps(dimensionNum, times))
            #getting info for next set of extremes
            info = dimfile.readline().split()
        
        #close current dimension file and iterate to next one
        dimfile.close()
    return dimensions, agendas

        
"""
countWordOccurences goes through each transcript line by line and,
if that line's timestamps span a timeopint on some agenda, 
saves all words on that line in the appropriate dictionary
"""
def countWordOccurences(dimensions, agendas):
    for thingsToLookFor in agendas:
        transcriptText = readTranscript(transcriptsDir + thingsToLookFor.filename + ".cha");
        for line in range(0, len(transcriptText)-1, 2):
            bounds = [int(i) for i in transcriptText[line+1].split("_")]  # utterance start and end times
            words = re.split("\*A:|\*B:|\*A1:|\*A2:|\*A3:|\*B1:|\*B2:|\*B3:| |:|,|", transcriptText[line])
            #checking if utterance is left or right
            if "*A" in transcriptText[line][:3]:
                for timestamp in thingsToLookFor.timestampInstances:
                    #searching for timestamp in whole list
                    timestamp.points[0], resultLL =search(bounds,timestamp.points[0])
                    timestamp.points[1], resultLH = search(bounds,timestamp.points[1])
                    dimInvolved = timestamp.dimInd
                   
                    #if found,insert into the respective dictionary
                    for word in words:
                        if len(word) > 0:
                            dimensions[dimInvolved].allWords[word] = \
                                dimensions[dimInvolved].allWords.get(word, 0) + 1
                            if resultLL:
                                dimensions[dimInvolved].leftLow[word] = \
                                    dimensions[dimInvolved].leftLow.get(word, 0) + 1
                            if resultLH:
                                dimensions[dimInvolved].leftHigh[word] = \
                                    dimensions[dimInvolved].leftHigh.get(word, 0) + 1
                                
            #all right speaker operations
            else:
              for timestamp in thingsToLookFor.timestampInstances:
                  #searching for timestamp in the whole list
                  timestamp.points[2], resultRL = search(bounds,timestamp.points[2])
                  timestamp.points[3], resultRH = search(bounds,timestamp.points[3])
                  dimInvolved = timestamp.dimInd
                  
                  #if found, insert into the respective dictionary
                  for word in words:
                      if len(word) > 0:
                          dimensions[dimInvolved].allWords[word] = \
                              dimensions[dimInvolved].allWords.get(word, 0) + 1
                          if resultRL:
                              dimensions[dimInvolved].rightLow[word] = \
                                  dimensions[dimInvolved].rightLow.get(word, 0) + 1
                          if resultRH:
                              dimensions[dimInvolved].rightHigh[word] = \
                                  dimensions[dimInvolved].rightHigh.get(word, 0) + 1

"""
analyze goes into every dimension and makes combinations of our dictionaries,
does probability analyses, calculates the ratio for each word in the combinations,
and writes the result in a text file.
These names were chosen as mnemonics based on preliminary listening to extreme timepoints plus
reference to the loadings.  They are not precise, nor even necessarily correct.
"""
def analyze(dimensions):
    for dimNum in range(1, len(dimensions)):  
        """
        Merging the dictionaries
        If it's Dim 4,7, or 10 the low hashes and high hashes are grouped together,
          since the two sides behave the same (by inspection of the loadings)
        Otherwise the two sides behave reciprocally, so group crosswise
        """
        if dimNum in [4,7,10]:
            combinations = [merge(dimensions[dimNum].leftHigh, dimensions[dimNum].rightHigh), \
                            merge(dimensions[dimNum].leftLow, dimensions[dimNum].rightLow)]
        else:
            combinations = [merge(dimensions[dimNum].leftHigh, dimensions[dimNum].rightLow), \
                            merge(dimensions[dimNum].leftLow, dimensions[dimNum].rightHigh)]
            
        combinationTotals = [sum(combinations[i].values()) for i in range(2)]
        totalWords = sum(dimensions[dimNum].allWords.values())
        print("writing output for dimension", dimNum)
            
        for combNum in range(2):
            dimSideName = investigationNames[((dimNum-1) * 2) + combNum]
            countFileName = "countsDim%02d%s.txt" % (dimNum, dimSideName) 
            resultFile = io.open(countFileName, "w", encoding = "utf-8")
            resultFile.write(dimensions[dimNum].filename[:-4] + " - " +  dimSideName  + ":\n")
            DictCopy = copy.deepcopy(dimensions[dimNum].allWords)
            
            for word, localOccurences in combinations[combNum].items():
                globalOccurences = dimensions[dimNum].allWords.get(word, 0)
                thisWordHere = localOccurences
                thisWordElsewhere = globalOccurences - localOccurences
                otherWordsHere = combinationTotals[combNum] - localOccurences
                
                otherWordsElsewhere =totalWords - thisWordHere - thisWordElsewhere - otherWordsHere
                contingencyTable =[[thisWordHere, thisWordElsewhere], \
                                   [otherWordsHere, otherWordsElsewhere]]
                
                #if our sample is large enough, do chi square
                if get_lowest_expected(contingencyTable) > 5:
                    chi2, prob, dof, expected = chi2_contingency(contingencyTable)
                #otherwise do fishers exact test
                else:
                    oddsratio, prob = fisher_exact(contingencyTable)
                    if prob >= lowFreqThreshold: 
                        continue
                        
                ratio = (localOccurences/combinationTotals[combNum])/ (globalOccurences/totalWords)
                    
                if prob < highFreqThreshold:
                    #Print the word, p value and ratio of local occurences to occurences in whole file
                    outline = "Word: %5s \t p-value: %.4f,  ratio: %.2f  " % (word, prob, ratio)
                    moreOutput = "( = (%d/%d) / (%d/%d)\n" % \
                    (localOccurences,combinationTotals[combNum], globalOccurences, totalWords)
                    resultFile.write(outline+moreOutput) 
                DictCopy.pop(word)
            

            #Now examine the words that never appeared in the  combination
            for word, occurences in DictCopy.items():
                """
                Contingency table: 
                                            In Dictionary                                 In rest of File
                word occurences                 0                                           occurences
                non-word occurences    combinationTotals[combNum]           (totalWords - all other entries in table)
                """
                contingencyTable = [[0, occurences], \
                                    [combinationTotals[combNum], totalWords]]
                contingencyTable[1][1] -= contingencyTable[0][1] + contingencyTable[1][0]
                
                #if our sample is large enough, do chi square
                if get_lowest_expected(contingencyTable) > 5:
                    chi2, prob, dof, expected = chi2_contingency(contingencyTable)
                #otherwise do fishers exact test
                else:
                    oddsratio, prob = fisher_exact(contingencyTable)
                    
                if prob < lowFreqThreshold:
                    ratio = 0
                    line = "Word: " + word + "  \tp-value: {:.4f}\tratio: {:.2f}\n"
                    resultFile.write(line.format(prob, ratio))
            #separate the dimensions with new lines and loop back to the new dimension
        resultFile.write("\n")
        resultFile.close()
           
                   
"""
Search iterates through our array of searchees, looking for one that matches the bounds. 
If it's found, it returns a version of the searchee list without the element
and all the elements that come before it, so that subsequent searches go a little faster. 
Ex: searchee = [1.5, 2.5, 3.5, 4.5, 5.5] bounds = [3,4]
Result: searchee returned as [4.5, 5.5] and True because it was found
"""      
       
def search(bounds, searchee):
    for i in range(len(searchee)):
        if searchee[i] < bounds[0]:
            continue
            #print("too low", searchee[i],"  ",bounds[0])
        elif searchee[i] < bounds[1]:
            return searchee[i+1:], True
        else:
            #print("too high", searchee[i]," ", bounds[0]," ", bounds[1])
            return searchee, False
    return searchee, False           
        
"""
returns an array with the even indices having the words
and the odd indices having the corresponding timestamp
Ex: A[0]= "A*: Whatever the speaker said"
    A[1] = 123456_125400
"""
def readTranscript(filename):
    with io.open(filename, "r", encoding = "utf-8") as f:
        # Skipping 9-line header
        for unneeded in range(9):
            f.readline();
            
        fileString = f.read()
        
        #Removing all invalid characters
        fileString = fileString.replace("\n", " ").replace("\t", " ") 
        fileString = fileString.replace("⇗","").replace("→"," ")
        fileString = fileString.replace("⇘"," ").replace("一"," ")
        fileString = fileString.replace("[", "").replace("]", "")
        #                                    "                     "
        fileString = fileString.replace(chr(8220),"").replace(chr(8221), "") 
        fileString = fileString.replace("!"," ")
        # splits by  which separates every time stamp from the phrases
        return fileString.split("")


# findTranscriptAgenda finds the object matching the filename, or 
# makes a new one if not found
def findTranscriptAgenda(agendas, name):
    for thingsToLookFor in agendas:
        if thingsToLookFor.filename == name:
            return thingsToLookFor
        
    agendas.append(TranscriptAgenda(name))
    return agendas[-1]

#merge gets all the elements in dictionary B and adds them to dictionary A
def merge(dictA, dictB):
    for word, occurences in dictB.items():
        dictA[word] = dictA.get(word, 0) + occurences
    return dictA


#used to get the lowest value expected to see if it is viable to use chi_square
def get_lowest_expected(A):
    #getting the sums of each row and column and storing them
    rows = [A[0][0] + A[0][1], A[1][0] + A[1][1]] 
    cols = [A[0][0] + A[1][0], A[0][1] + A[1][1]]
    
    minimum = math.inf
    
    #for every combination of rows and columns
    for row in rows:
        for col in cols:
            product = row * col
            if product < minimum:
                minimum = product
                
    totalInContingencyTable = sum(rows)
    return minimum/totalInContingencyTable
    

# Main

print("Step one: Reading extremes files for each dimension (")
dimensions, agendas = readExtremeFiles()
print("Step two: Reading the transcripts and compiling counts for each word for each dimension")
countWordOccurences(dimensions, agendas)
print("Step three: Compute ratios and p-values and print the results to the results file")
analyze(dimensions)
print("All done")



"""        
Debug code fragments

print(dimensions)
for thingsToLookFor in agendas:
    print(thingsToLookFor.filename)
    for timestamp in thingsToLookFor.timestampInstances:
        print(timestamp.dimInd)
        print(timestamp.points)
        
Two functions below to handle printing of every dictionary created.
call traverse_dimensions(dimensions) to utilize it     

Note: if you wish to use the "traverse_dimensions" function to see 
the number of occurences for each word in every dictionary, you
must call "traverse_dimensions" before calling "analyze" because "analyze" 
as a side effect deletes items from the dictionaries. The results of
"traverse_dimensions" will show up in the same directory your file is saved in,
generating 5 files per dimension included. 

#Passes every dictionary in the list of dimensions to the writing function
def traverse_dimensions(dimensions):
    for dimension in dimensions[1:]:
        #adding offset to take off ".txt"
        offset = -4
        
        write(dimension.leftLow, dimension.filename[:offset] + "_LL_Results.txt")
        write(dimension.leftHigh, dimension.filename[:offset] + "_LH_Results.txt")
        write(dimension.rightLow, dimension.filename[:offset] + "_RL_Results.txt")
        write(dimension.rightHigh, dimension.filename[:offset] + "_RH_Results.txt")
        write(dimension.allWords, dimension.filename[:offset] + "_all_words.txt")
        
#Sorts hash by occurences and prints to a file
def write(dictionary, filename):
    output = io.open(filename,"w",encoding ="utf-8")
    #for every instance in our dictionary sorted descending by occurences
    for keyVal in sorted(dictionary.items(), key = lambda dictionary:(dictionary[1], dictionary[0]), reverse = True):
        word = keyVal[0]
        occurences = keyVal[1]
        
        #layout the format of our output and write it into the file
        printString = "Word: {}\tOccurences: {}\n"
        output.write(printString.format(word, occurences))
    output.close()
"""
